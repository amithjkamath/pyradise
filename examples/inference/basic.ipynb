{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DL-Model Inference Pipeline\n",
    "\n",
    "In this example the construction of an end-to-end DICOM-based auto-segmentation solution using the famous [U-Net](https://arxiv.org/abs/1505.04597) is demonstrated. The given solution delineates the skull of the patient based on a T1-weighted post-contrast and a T2-weighted image. For this example the provided example data and a given PyTorch-based DL-model is used that both can be found in the [example data GitHub repository](https://github.com/ruefene/pyradise-example-data).\n",
    "\n",
    "Because PyRaDiSe is DL-framework agnostic to allow for maximum flexibility, [PyTorch](https://pytorch.org/get-started/locally/#start-locally) must be installed to execute this example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation\n",
    "\n",
    "Before getting started with constructing the auto-segmentation solution one needs to import the following packages and modules."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Optional)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import pyradise.data as ps_data\n",
    "import pyradise.fileio as ps_io\n",
    "import pyradise.process as ps_proc\n",
    "\n",
    "from network import UNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## InferenceFilter Implementation\n",
    "\n",
    "In the following section, the implementation of a PyTorch-based inference filter is demonstrated. This implementation may be used as a starting point for more sophisticated inference filters. Implementation details are mentioned in the code below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class ExampleInferenceFilter(ps_proc.InferenceFilter):\n",
    "    \"\"\"An example implementation of an InferenceFilter for\n",
    "    2D segmentation with a PyTorch-based U-Net.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the device on which the model should be run\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Define a class attribute for the model\n",
    "        self.model: Optional[nn.Module] = None\n",
    "\n",
    "    def _prepare_model(self,\n",
    "                       model: nn.Module,\n",
    "                       model_path: str\n",
    "                       ) -> nn.Module:\n",
    "        \"\"\"Implementation using the PyTorch framework.\"\"\"\n",
    "\n",
    "        # Load model parameters\n",
    "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "\n",
    "        # Assign the model to the class\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _infer_on_batch(self,\n",
    "                        batch: Dict[str, Any],\n",
    "                        params: ps_proc.InferenceFilterParams\n",
    "                        ) -> Dict[str, Any]:\n",
    "        \"\"\"Implementation using the PyTorch framework.\"\"\"\n",
    "\n",
    "        # Stack and adjust the numpy array such that it fits the\n",
    "        # [batch, channel / images, height, width, (depth)] format\n",
    "        # Note: The following statement works for slice-wise and patch-wise processing\n",
    "        if (loop_axis := params.indexing_strategy.loop_axis) is None:\n",
    "            adjusted_input = np.stack(batch['data'], axis=0)\n",
    "        else:\n",
    "            adjusted_input = np.stack(batch['data'], axis=0).squeeze(loop_axis + 2)\n",
    "\n",
    "        # Generate a tensor from the numpy array\n",
    "        input_tensor = torch.from_numpy(adjusted_input)\n",
    "\n",
    "        # Move the batch to the same device as the model\n",
    "        input_tensor = input_tensor.to(self.device, dtype=torch.float32)\n",
    "\n",
    "        # Apply the model to the batch\n",
    "        with torch.no_grad():\n",
    "            output_tensor = self.model(input_tensor)\n",
    "\n",
    "        # Retrieve the predicted classes from the output\n",
    "        final_activation_fn = nn.Sigmoid()\n",
    "        output_tensor = (final_activation_fn(output_tensor) > 0.5).bool()\n",
    "\n",
    "        # Convert the output to a numpy array\n",
    "        # Note: The output shape must be [batch, height, width, (depth)]\n",
    "        output_array = output_tensor.cpu().numpy()\n",
    "\n",
    "        # Construct a list of output arrays such that it fits the index expressions\n",
    "        batch_output_list = [output_array[i, ...] for i in range(output_array.shape[0])]\n",
    "\n",
    "        # Combine the output arrays into a dictionary\n",
    "        output = {'data': batch_output_list,\n",
    "                  'index_expr': batch['index_expr']}\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter Pipeline Construction\n",
    "\n",
    "In this section, the construction of the processing pipeline is shown using the inference filter implemented before.\n",
    "\n",
    "This demonstrated processing pipeline is simple and does not include registration to a reference image that would modify the spatial properties of the input images. Thus, the playback of the transform tapes recoding the changes of the spatial properties is not required. However, in DL practice registration to a reference image is often used and a playback of the transform tapes is essential to generate correctly aligned segmentations. For those cases we recommend to add a [PlaybackTransformTapeFilter](reference/pyradise.process.invertibility.rst) to the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_pipeline(model_path: str) -> ps_proc.FilterPipeline:\n",
    "    # Construct a pipeline the processing\n",
    "    pipeline = ps_proc.FilterPipeline()\n",
    "\n",
    "    # Construct and ddd the preprocessing filters to the pipeline\n",
    "    output_size = (256, 256, 256)\n",
    "    output_spacing = (1.0, 1.0, 1.0)\n",
    "    reference_modality = 'T1'\n",
    "    resample_filter_params = ps_proc.ResampleFilterParams(output_size,\n",
    "                                                          output_spacing,\n",
    "                                                          reference_modality=reference_modality,\n",
    "                                                          centering_method='reference')\n",
    "    resample_filter = ps_proc.ResampleFilter()\n",
    "    pipeline.add_filter(resample_filter, resample_filter_params)\n",
    "\n",
    "    norm_filter_params = ps_proc.ZScoreNormFilterParams()\n",
    "    norm_filter = ps_proc.ZScoreNormFilter()\n",
    "    pipeline.add_filter(norm_filter, norm_filter_params)\n",
    "\n",
    "    # Construct and add the inference filter\n",
    "    modalities_to_use = ('T1', 'T2')\n",
    "    inf_params = ps_proc.InferenceFilterParams(model=UNet(num_channels=2, num_classes=1),\n",
    "                                               model_path=model_path,\n",
    "                                               modalities=modalities_to_use,\n",
    "                                               reference_modality=reference_modality,\n",
    "                                               output_organs=(ps_data.Organ('Skull'),),\n",
    "                                               output_rater=ps_data.Rater('AutoSegmentation'),\n",
    "                                               organ_indices=(1,),\n",
    "                                               batch_size=8,\n",
    "                                               indexing_strategy=ps_proc.SliceIndexingStrategy(0))\n",
    "\n",
    "    inf_filter = ExampleInferenceFilter()\n",
    "    pipeline.add_filter(inf_filter, inf_params)\n",
    "\n",
    "    # Add postprocessing filters\n",
    "    cc_filter_params = ps_proc.SingleConnectedComponentFilterParams()\n",
    "    cc_filter = ps_proc.SingleConnectedComponentFilter()\n",
    "    pipeline.add_filter(cc_filter, cc_filter_params)\n",
    "\n",
    "    # Because the spatial properties of the subject images are\n",
    "    # changed with respect to the reference T1 image a playback\n",
    "    # of the TransformTape is not required. If the spatial properties\n",
    "    # of the reference image would have been changed the playback can\n",
    "    # be achieved using the PlaybackTransformTapeFilter.\n",
    "    #\n",
    "    # playback_params = PlaybackTransformTapeFilterParams()\n",
    "    # playback_filter = PlaybackTransformTapeFilter()\n",
    "    # pipeline.add_filter(playback_filter, playback_params)\n",
    "\n",
    "    return pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auto-segmentation Pipeline Construction\n",
    "\n",
    "The following section demonstrates the construction of the inference procedure that can be split into the following tasks:\n",
    "\n",
    "- Import DICOM images\n",
    "- Generate and run the filter pipeline\n",
    "- Convert segmentation masks to DICOM-RTSS\n",
    "- Serialize DICOM-RTSS and copy the original DICOM images\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def infer(input_dir_path: str,\n",
    "          output_dir_path: str,\n",
    "          model_path: str\n",
    "          ) -> None:\n",
    "    # Crawl the data in the input directory\n",
    "    crawler = ps_io.SubjectDicomCrawler(input_dir_path)\n",
    "    series_info = crawler.execute()\n",
    "\n",
    "    # Select the required modalities\n",
    "    used_modalities = ('T1', 'T2')\n",
    "    modality_selector = ps_io.ModalityInfoSelector(used_modalities)\n",
    "    series_info = modality_selector.execute(series_info)\n",
    "\n",
    "    # Exclude the existing DICOM-RTSS files\n",
    "    no_rtss_selector = ps_io.NoRTSSInfoSelector()\n",
    "    series_info = no_rtss_selector.execute(series_info)\n",
    "\n",
    "    # Construct the loader and load the subject\n",
    "    loader = ps_io.SubjectLoader()\n",
    "    subject = loader.load(series_info)\n",
    "\n",
    "    # Construct the pipeline and execute it\n",
    "    pipeline = get_pipeline(model_path)\n",
    "    subject = pipeline.execute(subject)\n",
    "\n",
    "    # Define the customizable metadata for the DICOM-RTSS\n",
    "    # Note: Check the value formatting at:\n",
    "    # https://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html\n",
    "    meta_data = ps_io.RTSSMetaData(patient_name='Jack Demo',\n",
    "                                   patient_id=subject.get_name(),\n",
    "                                   patient_birth_date='19700101',\n",
    "                                   patient_sex='F',\n",
    "                                   patient_weight='80',\n",
    "                                   patient_size='180',\n",
    "                                   series_description='Demo Series Description',\n",
    "                                   series_number='10',\n",
    "                                   operators_name='Auto-Segmentation Alg.')\n",
    "\n",
    "    # Convert the segmentations to a DICOM-RTSS\n",
    "    reference_modality = 'T1'\n",
    "    conv_conf = ps_io.RTSSConverter3DConfiguration(decimate_reduction=0.5)\n",
    "    converter = ps_io.SubjectToRTSSConverter(subject,\n",
    "                                             series_info,\n",
    "                                             reference_modality,\n",
    "                                             conv_conf,\n",
    "                                             meta_data)\n",
    "    rtss_dataset = converter.convert()\n",
    "\n",
    "    # Save the new DICOM-RTSS\n",
    "    named_rtss = (('rtss.dcm', rtss_dataset),)\n",
    "    writer = ps_io.DicomSeriesSubjectWriter()\n",
    "    writer.write(named_rtss,\n",
    "                 output_dir_path,\n",
    "                 subject.get_name(),\n",
    "                 series_info)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Auto-segmentation Pipeline Execution\n",
    "\n",
    "Now, the auto-segmentation pipeline is finished and can be executed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Adjust the input directory path accordingly. Make sure that the input path points\n",
    "# to a subject directory (e.g. //YOUR/PATH/VS-SEG-001).\n",
    "input_path = 'D:/example_data/dicom_data/VS-SEG-001'\n",
    "\n",
    "# Adjust the model path accordingly.\n",
    "model_path_ = 'D:/example_data/model/model.pth'\n",
    "\n",
    "# Adjust the output directory path accordingly and\n",
    "# make sure the output directory is empty.\n",
    "output_path = 'D:/example_output/basic_inference'\n",
    "\n",
    "# Execute the inference procedure\n",
    "infer(input_path, output_path, model_path_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Result\n",
    "\n",
    "After execution of the pipeline, the resulting DICOM data in the output directory can be examined using a DICOM viewer such as [3DSlicer](https://www.slicer.org/).\n",
    "\n",
    "<p align=\"center\"><img src=\"../examples/inference/images/inference_0.png\"  width=\"500\"></p>\n",
    "\n",
    "The 3D reconstruction of the predicted skull as displayed by 3DSlicer.\n",
    "\n",
    "<p align=\"center\"><img src=\"../examples/inference/images/inference_2_ax.png\"  width=\"500\"></p>\n",
    "\n",
    "Overlay of the predicted skull segmentation on the T1-weighted image viewed on the axial plane.\n",
    "\n",
    "<p align=\"center\"><img src=\"../examples/inference/images/inference_2_cor.png\"  width=\"500\"></p>\n",
    "\n",
    "Overlay of the predicted skull segmentation on the T1-weighted image viewed on the coronal plane.\n",
    "\n",
    "<p align=\"center\"><img src=\"../examples/inference/images/inference_2_sag.png\"  width=\"500\"></p>\n",
    "\n",
    "Overlay of the predicted skull segmentation on the T1-weighted image viewed on the sagittal plane."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
